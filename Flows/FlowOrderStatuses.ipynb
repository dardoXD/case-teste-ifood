{"cells":[{"cell_type":"markdown","source":["# Classe FlowOrderStatuses\n\nClasse que implementa o fluxo de Status por Order herdando da classe FlowBase."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, concat, regexp_replace, max, expr\nfrom pyspark.sql.types import IntegerType\nfrom datetime import datetime\n\nclass FlowOrderStatuses(FlowBase):\n    def run(self):\n        usedDatasets =['Order', 'OrderStatuses']\n        \n        orderDF = FlowBase.loadUsedDatasets('Order')\n        orderStatusesDF = FlowBase.loadUsedDatasets('OrderStatuses')\n        \n        orderStatusesDF = orderStatusesDF.withColumn('CONCLUDED', expr('''\n            CASE WHEN value == 'CONCLUDED' THEN created_at\n            ELSE NULL\n            END AS CONCLUDED\n        '''))\n        \n        orderStatusesDF = orderStatusesDF.withColumn('REGISTERED', expr('''\n            CASE WHEN value == 'REGISTERED' THEN created_at\n            ELSE NULL\n            END AS REGISTERED\n        '''))\n        \n        orderStatusesDF = orderStatusesDF.withColumn('CANCELLED', expr('''\n            CASE WHEN value == 'CANCELLED' THEN created_at\n            ELSE NULL\n            END AS CANCELLED\n        '''))\n        \n        orderStatusesDF = orderStatusesDF.withColumn('PLACED', expr('''\n            CASE WHEN value == 'PLACED' THEN created_at\n            ELSE NULL\n            END AS PLACED\n        '''))\n        \n        orderStatusesDF.registerTempTable(\"df_table\")\n        orderStatusesDF = spark.sql(\"SELECT order_id, MAX(CONCLUDED) as CONCLUDED, MAX(REGISTERED) as REGISTERED, MAX(CANCELLED) as CANCELLED, MAX(PLACED) as PLACED FROM df_table GROUP BY order_id\")\n        \n        df_final = orderDF.join(orderStatusesDF, (orderDF.order_id == orderStatusesDF.order_id)).drop(orderStatusesDF.order_id)\n        \n        if self.checkDuplicates(df_final):\n            dbutils.notebook.exit('ERROR: Existem linhas duplicadas')\n            \n        if self.checkColumnsNull(df_final, ['cpf', 'order_id']):\n            dbutils.notebook.exit('ERROR: Existem chaves com valores nulos')\n        \n        spark_udf = udf(FlowBase.encrypt_value, StringType())\n        df_final = df_final.withColumn('cpf', spark_udf(col('cpf')))\n        \n        return df_final"],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"FlowOrderStatuses","notebookId":3104674543945717},"nbformat":4,"nbformat_minor":0}
