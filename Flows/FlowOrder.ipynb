{"cells":[{"cell_type":"markdown","source":["# Classe FlowOrder\n\nClasse que implementa o fluxo com todas as informações de Orders herdando da classe FlowBase."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, concat, regexp_replace, max, struct, lit\nfrom pyspark.sql.types import IntegerType, StringType\nfrom datetime import datetime\nimport hashlib\n\nclass FlowOrder(FlowBase):\n    def run(self):\n        usedDatasets =['Order', 'OrderStatuses', 'Consumer', 'Restaurant']\n        \n        orderDF = FlowOrder.loadUsedDatasets('Order')\n        \n        orderStatusesDF = FlowOrder.loadUsedDatasets('OrderStatuses')\n        consumerDF = FlowOrder.loadUsedDatasets('Consumer')\n        restaurantDF = FlowOrder.loadUsedDatasets('Restaurant')\n        consumerDF = consumerDF.withColumnRenamed('created_at', 'consumer_created_at')\n        \n        df_final = orderDF.join(consumerDF, orderDF.customer_id == consumerDF.customer_id, how='left').drop(consumerDF.customer_id).drop(consumerDF.customer_name)\n        \n        df_final = df_final.join(restaurantDF, df_final.merchant_id == restaurantDF.id, how='left').drop(restaurantDF.id)\n        \n        orderStatusesDF = orderStatusesDF.withColumn('converted_date', regexp_replace(col('created_at'), '\\D', ''))\n        orderStatusesDF = orderStatusesDF.withColumn('converted_date', orderStatusesDF.converted_date.cast('long'))\n        \n        df = orderStatusesDF\n        \n        df = df.groupBy('order_id').agg(max('converted_date').alias('converted_date'))\n        \n        df = df.join(orderStatusesDF, (df.order_id == orderStatusesDF.order_id) & (df.converted_date == orderStatusesDF.converted_date), how='left').drop(orderStatusesDF.order_id).drop(df.converted_date).drop(orderStatusesDF.converted_date)\n        \n        df = df.withColumnRenamed('created_at', 'status_created_at')\n        \n        df = df.dropDuplicates((['order_id']))\n        \n        df_final = df_final.join(df, df_final.order_id == df.order_id, how='left').drop(df.order_id)\n        \n        if self.checkDuplicates(df_final):\n            dbutils.notebook.exit('ERROR: Existem linhas duplicadas')\n            \n        if self.checkColumnsNull(df_final, ['cpf', 'order_id']):\n            dbutils.notebook.exit('ERROR: Existem chaves com valores nulos')\n        \n        df_final.cache()   \n        spark_udf = udf(FlowBase.encrypt_value, StringType())\n        \n        df_final = df_final.withColumn('cpf', spark_udf(col('cpf')))\n        df_final = df_final.withColumn('customer_name', spark_udf(col('customer_name')))\n        \n        return df_final"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2}],"metadata":{"name":"FlowOrder","notebookId":4001667649767252},"nbformat":4,"nbformat_minor":0}
