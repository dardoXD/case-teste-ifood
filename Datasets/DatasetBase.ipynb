{"cells":[{"cell_type":"markdown","source":["# Classe DatasetBase\n\nEssa classe é responsável por criar o objeto que vai conter as informações dos Datasets criados.\n\n## Métodos:\n* __init__: Construtor da classe onde são passados os seguintes parâmetros\n  * bucket_name_input: Nome do bucket onde está o dataset.\n  * path_input: Caminho do arquivo no bucket.\n  * delimiter_input: Delimitador utilizado no arquivo.\n  * extension: Extensão utilizada no arquivo.\n  \n  \n* **loadDatabase**: Método para criar o bucket, ler o arquivo e retornar o Dataframe referente. Utiliza os seguintes parâmetros:\n  * bucket_name: Nome do bucket onde está o dataset.\n  * complete_path: Caminho completo do arquivo no sistema de arquivos do Databricks.\n  * extension: Extensão utilizada no arquivo.\n  * delimiter: Delimitador utilizado no arquivo.\n  \n* **writeDataset**: Método para criar o bucket e escrever um Dataframe em formato parquet no bucket informado. Utiliza os seguintes parâmetros:\n  * df: Dataframe que será utilizado para escrever no caminho informado.\n  * bucket_name_output: Nome do bucket onde será escrito o dataset.\n  * complete_path_output: Caminho completo onde arquivo será escrito no sistema de arquivos do Databricks.\n  \n* **createBucket**: Método que cria o bucket AWS no sistema de arquivos do Databricks. Utiliza os seguintes parâmetros:\n  * bucket_name: Nome do bucket que será carregado.\n  \n* **deleteBucket**: Método que deleta o bucket AWS do sistema de arquivos do Databricks. Utiliza os seguintes parâmetros:\n  * bucket_name: Nome do bucket que será deletado."],"metadata":{}},{"cell_type":"code","source":["class DatasetBase:\n    def __init__(self, bucket_name_input, path_input, delimiter_input, extension):\n        self.bucket_name_input = bucket_name_input\n        self.path_input = path_input\n        self.delimiter_input = delimiter_input\n        self.extension = extension\n     \n    @staticmethod\n    def loadDatabase(bucket_name, complete_path, extension='parquet', delimiter=','):\n        DatasetBase.createBucket(bucket_name)\n        \n        if extension == 'csv':\n            df = spark.read.options(header=True,delimiter=delimiter).csv(complete_path)\n        elif extension == 'json':\n            df = spark.read.json(complete_path)\n        elif extension == 'parquet':\n            df = spark.read.parquet(complete_path)\n        else:\n            raise 'ERROR: Formato não compatível'\n            \n        return df\n    \n    @staticmethod\n    def writeDataset(df, bucket_name_output, complete_path_output):\n        DatasetBase.createBucket(bucket_name_output)\n        df.write.format('parquet').mode(\"overwrite\").save(complete_path_output)\n        return df\n      \n    @staticmethod\n    def createBucket(bucket_name):\n        access_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-access-key\")\n        secret_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-secret-key\").replace(\"/\", \"%2F\")\n        encription = \"sse-s3\"\n        \n        if not any(mount.mountPoint == f'/mnt/{bucket_name}' for mount in dbutils.fs.mounts()):\n            dbutils.fs.mount(f's3a://{access_key}:{secret_key}@{bucket_name}', f'/mnt/{bucket_name}', encription)            \n        return dbutils.fs.mounts()\n    \n    @staticmethod\n    def deleteBucket(bucket_name):\n        if any(mount.mountPoint == f'/mnt/{bucket_name}' for mount in dbutils.fs.mounts()):\n            dbutils.fs.unmount(f'/mnt/{bucket_name}')        \n        return dbutils.fs.mounts()"],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"DatasetBase","notebookId":4001667649767247},"nbformat":4,"nbformat_minor":0}
