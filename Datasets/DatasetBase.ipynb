{"cells":[{"cell_type":"code","source":["class DatasetBase:\n    def __init__(self, bucket_name_input, path_input, delimiter_input, extension):\n        self.bucket_name_input = bucket_name_input\n        self.path_input = path_input\n        self.delimiter_input = delimiter_input\n        self.extension = extension\n        \n    def setVariables(self):\n        self.complete_path_input = '/mnt/' + self.bucket_name_input + self.path_input\n        self.filename_input = self.path_input.split('/')[-1].split('.')[0]\n        self.extension_input = self.path_input.split('/')[-1].split('.')[1]\n        \n        if self.path_input.split('/')[-1].split('.')[-1] == self.extension_input:\n            self.compression_input = ''\n        else:\n            self.compression_input = self.path_input.split('/')[-1].split('.')[2]\n            \n        self.bucket_name_output = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-name\")\n        self.complete_path_output = '/mnt/' + self.bucket_name_output + '/raw/' + self.filename_input + '/' \n     \n    @staticmethod\n    def loadDatabase(bucket_name, complete_path, extension='parquet', delimiter=','):\n        DatasetBase.createBucket(bucket_name)\n        \n        if extension == 'csv':\n            df = spark.read.options(header=True,delimiter=delimiter).csv(complete_path)\n        elif extension == 'json':\n            df = spark.read.json(complete_path)\n        elif extension == 'parquet':\n            df = spark.read.parquet(complete_path)\n        else:\n            raise 'ERROR: Formato não compatível'\n            \n        return df\n    \n    @staticmethod\n    def writeDataset(df, bucket_name_output, complete_path_output):\n        DatasetBase.createBucket(bucket_name_output)\n        df.write.format('parquet').mode(\"overwrite\").save(complete_path_output)\n        return df\n      \n    @staticmethod\n    def createBucket(bucket_name):\n        access_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-access-key\")\n        secret_key = dbutils.secrets.get(scope = \"aws\", key = \"aws-secret-key\").replace(\"/\", \"%2F\")\n        encription = \"sse-s3\"\n        \n        if not any(mount.mountPoint == f'/mnt/{bucket_name}' for mount in dbutils.fs.mounts()):\n            dbutils.fs.mount(f's3a://{access_key}:{secret_key}@{bucket_name}', f'/mnt/{bucket_name}', encription)            \n        return dbutils.fs.mounts()\n    \n    @staticmethod\n    def deleteBucket(bucket_name):\n        if any(mount.mountPoint == f'/mnt/{bucket_name}' for mount in dbutils.fs.mounts()):\n            dbutils.fs.unmount(f'/mnt/{bucket_name}')        \n        return dbutils.fs.mounts()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[59]: True</div>"]}}],"execution_count":1}],"metadata":{"name":"DatasetBase","notebookId":4001667649767247},"nbformat":4,"nbformat_minor":0}
