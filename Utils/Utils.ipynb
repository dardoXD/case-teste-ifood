{"cells":[{"cell_type":"markdown","source":["# Arquivo Utils\n\nBiblioteca com algumas funções que podem ser úteis para o desenvolvimento dentro do projeto.\n\n## Funções:\n* **file_exists**: Função que verifica se um arquivo existe no sistema de arquivos do Databricks. Utiliza o seguinte parâmetro:\n  * path: Caminho completo do arquivo no sistema de arquivos do Databricks.\n  \n* **get_configs_dataset**: Função que pega as configurações dos datasets cadastrados utilizando o nome do Dataset. Utiliza o seguinte parâmetros:\n  * name: Nome do Dataset cadastrado."],"metadata":{}},{"cell_type":"code","source":["def file_exists(path):\n    try:\n        dbutils.fs.ls(path)\n        return True\n    except Exception as e:\n        return False"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.functions import lower, col\n\ndef get_configs_dataset(name):\n    bucket_write = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-name\")\n    complete_path = f'/mnt/{bucket_write}/raw/datasets'\n    \n    DatasetBase.createBucket(bucket_write)\n    dataFrame = DatasetBase.loadDatabase(bucket_write, complete_path)\n    id_dataset, name, bucket_name, path, delimiter, extension, created_at = dataFrame.filter(lower(col('name')) == name).collect()[0]\n    \n    return id_dataset, name, bucket_name, path, delimiter, extension, created_at"],"metadata":{},"outputs":[],"execution_count":3}],"metadata":{"name":"Utils","notebookId":3834816749215802},"nbformat":4,"nbformat_minor":0}
