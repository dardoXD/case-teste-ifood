{"cells":[{"cell_type":"markdown","source":["# Arquivo de Execução para Carregar os Datasets na AWS\n\n* Argumentos:\n  * dataset: Nome do(s) dataset(s) que serão carregados separados por ','"],"metadata":{}},{"cell_type":"markdown","source":["Criação dos widgets que receberão os argumentos"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"datasets\", \"\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Atribuição dos valores dos argumentos às variáveis"],"metadata":{}},{"cell_type":"code","source":["datasets = dbutils.widgets.get(\"datasets\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Importação da classe de DatasetBase"],"metadata":{}},{"cell_type":"code","source":["%run ../Datasets/DatasetBase"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["* Criação do Bucket que armazena os arquivos com os Datasets cadastrados na AWS.\n* Leitura dos arquivos com os Datasets cadastrados.\n* Exibição dos Datasets cadastrados."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lower, col\ndatasets = str(datasets).replace(' ','').lower().split(',')\n\nbucket_write = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-name\")\nDatasetBase.createBucket(bucket_write)\n\ncomplete_path = f'/mnt/{bucket_write}/raw/datasets'\n\ndatasetsDataFrame = DatasetBase.loadDatabase(bucket_write, complete_path)\ndisplay(datasetsDataFrame)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["* Criação dos buckets utilizados\n* Leitura do arquivo no source\n* Escrita na AWS dos datasets em parquet passados como argumento\n\nObs.: Para melhorar a performance, poderia ser utilizada uma política de carregamento de Delta dos arquivos"],"metadata":{}},{"cell_type":"code","source":["from datetime import datetime\n\nfor dataset in datasets:\n    if datasetsDataFrame.filter(lower(col('name')) == dataset).count() == 1:\n        _, name, bucket_name, path, delimiter, extension, _ = datasetsDataFrame.filter(lower(col('name')) == dataset).collect()[0]\n        \n        newDataset = DatasetBase(bucket_name, path, delimiter, extension)\n        DatasetBase.createBucket(bucket_name)\n        \n        complete_path = f'/mnt/{bucket_name}/{path}'\n        newDataset.setVariables()\n        df = newDataset.loadDatabase(bucket_name, complete_path, extension)\n        \n        path_write = f'/mnt/{bucket_write}/raw/{name.lower()}/{datetime.now().strftime(\"%Y\")}/{datetime.now().strftime(\"%m\")}/{datetime.now().strftime(\"%d\")}'\n        DatasetBase.writeDataset(df, bucket_write, path_write)\n        \n        print(f'Dataset {name} carregado com Sucesso')\n    else:\n        print(f'Dataset {name} não cadastrado!!')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Exibição dos Arquivos gravados na AWS"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(f'/mnt/{bucket_write}/raw/'))"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"RunDatasets","notebookId":3834816749215806},"nbformat":4,"nbformat_minor":0}
