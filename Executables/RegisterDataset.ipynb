{"cells":[{"cell_type":"markdown","source":["# Arquivo de Execução para Registro de Datasets\n\n* Argumentos:\n  * bucket_name: Nome do bucket onde está o arquivo que será utilizado para carregar o Dataset\n  * delimiter: Delimitador utilizado no arquivo que será carregado para o Dataset\n  * extension: Extensão utilizada no arquivo que será carregado para o Dataset\n  * name: Nome do Dataset que será cadastrado\n  * path: Caminho onde está o arquivo que será utilizado para carregar o Dataset"],"metadata":{}},{"cell_type":"markdown","source":["Criação dos widgets que receberão os argumentos"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"name\", \"\")\ndbutils.widgets.text(\"bucket_name\", \"\")\ndbutils.widgets.text(\"path\", \"\")\ndbutils.widgets.text(\"delimiter\", \"\")\ndbutils.widgets.text(\"extension\", \"\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Atribuição dos valores dos argumentos às variáveis"],"metadata":{}},{"cell_type":"code","source":["name = dbutils.widgets.get(\"name\")\nbucket_name = dbutils.widgets.get(\"bucket_name\")\npath = dbutils.widgets.get(\"path\")\ndelimiter = dbutils.widgets.get(\"delimiter\")\nextension = dbutils.widgets.get(\"extension\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Importação da classe de DatasetBase"],"metadata":{}},{"cell_type":"code","source":["%run ../Datasets/DatasetBase"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Importação da biblioteca Utils"],"metadata":{}},{"cell_type":"code","source":["%run ../Utils/Utils"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["* Criação do Bucket que armanezará o arquivo que irá manter os cadastros dos Datasets.\n* Verificação se o Dataset já está cadastrado\n* Cadastro do novo Dataset se ainda não houver"],"metadata":{}},{"cell_type":"code","source":["bucket_write = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-name\")\nDatasetBase.createBucket(bucket_write)\ncount = 1\n\ncomplete_path = f'/mnt/{bucket_write}/raw/datasets'\n\nfrom datetime import datetime\nfrom pyspark.sql.functions import col, max\n\nif file_exists(complete_path):\n    oldDataFrame = DatasetBase.loadDatabase(bucket_write, complete_path)\n    \n    if oldDataFrame.filter(col('name') == name).count() < 1:\n        count = oldDataFrame.agg(max(col('id'))).collect()[0][0] + 1  \n        conf = [(count, name, bucket_name, path, delimiter, extension, datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))]\n        newDataFrame = spark.createDataFrame(conf, ['id', 'name', 'bucket_name', 'path', 'delimiter', 'extension', 'created_at'])\n        df = newDataFrame.union(oldDataFrame)\n    else:\n        dbutils.notebook.exit('Dataset já cadastrado!!')\nelse:\n    count = 1\n    conf = [(count, name, bucket_name, path, delimiter, extension, datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))]\n    df = spark.createDataFrame(conf, ['id', 'name', 'bucket_name', 'path', 'delimiter', 'extension', 'created_at'])\n\ntry:\n    DatasetBase.writeDataset(df, bucket_write, complete_path)\n    print('Dataset cadastrado com Sucesso!!!')\nexcept Exception:\n    dbutils.notebook.exit('ERROR: Erro na escrita do Dataset')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Exibição do Arquivo com todos os Datasets Cadastrados"],"metadata":{}},{"cell_type":"code","source":["df = DatasetBase.loadDatabase(bucket_name, complete_path)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"RegisterDataset","notebookId":1271276230614211},"nbformat":4,"nbformat_minor":0}
