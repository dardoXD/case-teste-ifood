{"cells":[{"cell_type":"code","source":["dbutils.widgets.text(\"name\", \"\")\ndbutils.widgets.text(\"bucket_name\", \"\")\ndbutils.widgets.text(\"path\", \"\")\ndbutils.widgets.text(\"delimiter\", \"\")\ndbutils.widgets.text(\"extension\", \"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["name = dbutils.widgets.get(\"name\")\nbucket_name = dbutils.widgets.get(\"bucket_name\")\npath = dbutils.widgets.get(\"path\")\ndelimiter = dbutils.widgets.get(\"delimiter\")\nextension = dbutils.widgets.get(\"extension\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%run ../Datasets/DatasetBase"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ../Utils/Utils"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["bucket_write = dbutils.secrets.get(scope = \"aws\", key = \"aws-bucket-name\")\nDatasetBase.createBucket(bucket_write)\ncount = 1\n\ncomplete_path = f'/mnt/{bucket_write}/raw/datasets'\n\nfrom datetime import datetime\nfrom pyspark.sql.functions import col, max\n\nif file_exists(complete_path):\n    oldDataFrame = DatasetBase.loadDatabase(bucket_write, complete_path)\n    \n    if oldDataFrame.filter(col('name') == name).count() < 1:\n        count = oldDataFrame.agg(max(col('id'))).collect()[0][0] + 1  \n        conf = [(count, name, bucket_name, path, delimiter, extension, datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))]\n        newDataFrame = spark.createDataFrame(conf, ['id', 'name', 'bucket_name', 'path', 'delimiter', 'extension', 'created_at'])\n        df = newDataFrame.union(oldDataFrame)\n    else:\n        dbutils.notebook.exit('Dataset jÃ¡ cadastrado!!')\nelse:\n    count = 1\n    conf = [(count, name, bucket_name, path, delimiter, extension, datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))]\n    df = spark.createDataFrame(conf, ['id', 'name', 'bucket_name', 'path', 'delimiter', 'extension', 'created_at'])\n\ntry:\n    DatasetBase.writeDataset(df, bucket_write, complete_path)\n    print('Dataset cadastrado com Sucesso!!!')\nexcept Exception:\n    dbutils.notebook.exit('ERROR: Erro na escrita do Dataset')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Dataset cadastrado com Sucesso!!!\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df = DatasetBase.loadDatabase(bucket_name, complete_path)\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>bucket_name</th><th>path</th><th>delimiter</th><th>extension</th><th>created_at</th></tr></thead><tbody><tr><td>2</td><td>OrderStatuses</td><td>ifood-data-architect-test-source</td><td>status.json.gz</td><td></td><td>json</td><td>2020-08-03T05:32:48.000Z</td></tr><tr><td>3</td><td>Consumer</td><td>ifood-data-architect-test-source</td><td>consumer.csv.gz</td><td>,</td><td>csv</td><td>2020-08-03T05:33:27.000Z</td></tr><tr><td>4</td><td>Restaurant</td><td>ifood-data-architect-test-source</td><td>restaurant.csv.gz</td><td>,</td><td>csv</td><td>2020-08-03T05:33:55.000Z</td></tr><tr><td>1</td><td>Order</td><td>ifood-data-architect-test-source</td><td>order.json.gz</td><td></td><td>json</td><td>2020-08-03T05:32:10.000Z</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"RegisterDataset","notebookId":1271276230614211},"nbformat":4,"nbformat_minor":0}
